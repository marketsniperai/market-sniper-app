Single-provider dependency (Polygon) Risk: API quota exhaustion, rate limiting, schema changes, or regional outages will cascade to partial/empty outputs. Gap: “Safe degradation” is declared, but not exhaustively defined per endpoint/feature. Guardrail: Per-capability circuit breakers with backoff, explicit per-section availability fields, and per-ticker fallback states. Cache last-known-good snapshots with TTL and provenance tags. Batch pipeline brittleness Risk: One ticker/data fetch failure halts entire batch or produces inconsistent universes. Guardrail: Per-ticker isolation, retry tiers, and granular task-level failure reporting. Emit a run-level manifest that lists processed/failed tickers and reasons. Secrets/env drift in Cloud Run Risk: Misconfigured env vars or rotated secrets cause silent failures. Guardrail: Boot-time config validation endpoint and health checks that assert presence/format of required secrets and API reachability. Contract drift between backend and app Risk: Changes to JSON fields break clients; ASSET fallback can mask stale data. Guardrail: Versioned JSON schema, strict contract tests, and backward-compatible field additions. UI should visualize schema version and data age. News/options partial outages Risk: Missing sections lead users to over-index on remaining context; bias skew. Guardrail: Prominent “Context incomplete” badges per section with explicit “do not infer” footers. Universe file corruption or desync Risk: Beta universe JSON malformed or outdated causes processing errors or misleading outputs. Guardrail: Validate the universe file with schema, checksum, and date-of-last-review; refuse run if invalid. Enterprise-readiness gaps Observability baseline is underspecified Missing: Centralized logs with correlation IDs per run/ticker, metrics (success/fail counts, latency, call rates), and alerting. Need: Structured logging (JSON), request tracing, run manifests, SLO dashboards (availability, freshness, contract compliance). Operational runbooks and SLOs Missing: Defined SLOs for “data freshness,” “batch completion window,” “ticker coverage,” and “degradation modes.” Need: Runbooks for Polygon failures, secret rotation, schema changes, and partial batch reruns. Data governance and provenance Missing: Provenance fields per datum (source, timestamp, plan/tier), cache retention, and audit trails. Need: Append-only decision logs with hash chaining, and cache catalogs with TTLs and invalidation rules. Testing discipline Missing: Contract tests for /dashboard and /analyze, chaos tests for provider outages, and universe mutation tests. Need: CI pipeline with schema validation, synthetic outage injection, and golden-file tests for outputs. Change management Missing: Controlled rollout and versioning of scoring parameters, friction thresholds, and labels. Need: Feature flags and config registries with audit history; canary runs before full batch. Operational safety and guardrails Legal-safe phrasing at risk via UI interpretation Risk: Even non-prescriptive fields can be read as advice if visual emphasis suggests action. Guardrail: Mandatory “context not guidance” watermark on signal cards, and consistent language: “bias,” “scenario,” “invalidations,” never “entry/exit/stop.” Degradation clarity Risk: Users misinterpret missing context as neutrality. Guardrail: Distinguish “Unavailable,” “Delayed,” and “Stale” with color coding and tooltips; suppress composite scores when key inputs are missing. Friction override transparency Risk: Override seen as contradictory rather than protective. Guardrail: Always show an explicit override banner with the driving factors and an “exposure restricted” rationale line. Bias due to fixed universe Risk: Narrow or static universes create survivorship and selection bias; users overgeneralize. Guardrail: Display “Universe scope” metadata per day (list, version, rationale); require periodic review logs. Effectiveness metrics misread as performance Risk: Users infer PnL. Guardrail: Keep effectiveness panel visually distinct, include explicit non-PnL disclaimer, and avoid terms like “win,” “loss,” or “profit factor” in frontend. Code and architecture smells Unversioned JSON contract as the “database” Smell: Tight coupling between output shape and UI; difficult to evolve fields safely. Fix: Introduce schema versioning and compatibility layers; maintain migration notes. Keep ASSET fallback time-bounded. Monolithic scoring with mixed concerns Smell: Direction, context, friction, overrides, and invalidations cohabiting can lead to tangled logic and implicit dependencies. Fix: Compose scoring via deterministic modules with explicit inputs/outputs; dependency graph documented; unit tests per module. Global configuration without provenance Smell: Hidden config changes alter outcomes with no audit trail. Fix: Config registry stored as versioned artifacts (hash + timestamp); include config snapshot in every decision record. Implicit error handling Smell: “No uncontrolled exceptions” goal without standardized error taxonomy. Fix: Define an error code catalog (e.g., DATA_UNAVAILABLE, SCHEMA_MISMATCH, RATE_LIMITED) and map to UI badges. Options/news adapters Smell: If adapters embed business phrasing, drift can reintroduce guidance language. Fix: Centralize lexicon in a “context-phrasing” module with lint checks that prohibit action verbs and recommendation terms. Universe processing order Smell: Hidden ordering effects (e.g., first-processed tickers benefiting from fresher caches) can bias day-level summaries. Fix: Randomize or explicitly fix order, record order in logs, and measure order impact. Minimal, high-impact fixes Contract schema versioning + UI compatibility layer Add schema_version to all responses; publish a changelog; fail-safe rendering for unknown fields. Run manifest + per-ticker audit packets Emit run.json with start/end, universe, config hash, and counts; per-ticker audit JSONL entries with reason codes and availability flags. Error taxonomy and degradation states Standardize error codes and map to UI badges; suppress composite scores when critical inputs are missing. Observability baseline Structured logs with run_id/ticker_id, metrics export (Prometheus/OpenTelemetry), and alerts for missing data or elevated friction override rates. Config snapshotting Include config hash and friction thresholds in each decision record; store snapshots for post-mortems. Chaos testing for provider outages Simulate rate limits, 500s, and schema changes in CI; verify graceful degradation and user messaging.